# Poker Assistant - Test Maintenance and Deployment Guide

## ğŸ¯ Overview

This guide provides comprehensive instructions for maintaining the Poker Assistant testing suite, integrating it into development workflows, and ensuring long-term test effectiveness.

## ğŸ“‹ Daily Development Workflow

### **Before Starting Development**
```bash
# Quick health check (2-3 minutes)
python final_test_validation.py --quick

# If working on specific components
python run_all_tests.py --unit          # Core logic changes
python run_all_tests.py --performance   # Performance-related changes
python run_all_tests.py --gui           # UI changes
```

### **During Development**
```bash
# Test specific functionality you're working on
python -m pytest poker_test.py::TestSpecificClass -v

# Quick smoke test after major changes
python poker_smoke.py --quick

# Performance check after optimization
python performance_benchmark_tests.py
```

### **Before Committing Code**
```bash
# Full validation (15-20 minutes)
python final_test_validation.py

# Generate coverage report
python run_all_tests.py --coverage

# Check for regressions
python run_all_tests.py --performance
```

## ğŸ”„ Continuous Integration Setup

### **GitHub Actions Workflow**

Create `.github/workflows/poker-tests.yml`:

```yaml
name: Poker Assistant Test Suite

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.8, 3.9, '3.10', '3.11']

    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v3
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-cov psutil coverage
        pip install -r requirements.txt
    
    - name: Environment validation
      run: python final_test_validation.py --env-only
    
    - name: Core tests
      run: python run_all_tests.py --unit --enhanced
    
    - name: Security validation
      run: python run_all_tests.py --security
    
    - name: Performance benchmarks
      run: python run_all_tests.py --performance
    
    - name: Integration tests
      run: python run_all_tests.py --integration
    
    - name: Smoke tests
      run: python poker_smoke.py
    
    - name: Coverage report
      run: python run_all_tests.py --coverage
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        fail_ci_if_error: true

  performance-regression:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    - name: Set up Python
      uses: actions/setup-python@v3
      with:
        python-version: '3.10'
    - name: Install dependencies
      run: |
        pip install pytest psutil
        pip install -r requirements.txt
    - name: Performance regression check
      run: python performance_benchmark_tests.py --regression-check

  final-validation:
    needs: [test, performance-regression]
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    - name: Set up Python
      uses: actions/setup-python@v3
      with:
        python-version: '3.10'
    - name: Install dependencies
      run: |
        pip install pytest pytest-cov psutil coverage
        pip install -r requirements.txt
    - name: Final validation
      run: python final_test_validation.py
    - name: Archive test results
      uses: actions/upload-artifact@v3
      with:
        name: test-results
        path: |
          validation_report_*.json
          poker_tests.log
          htmlcov/
```

### **Pre-commit Hooks**

Install pre-commit hooks to run tests automatically:

```bash
# Install pre-commit
pip install pre-commit

# Create .pre-commit-config.yaml
cat > .pre-commit-config.yaml << 'EOF'
repos:
- repo: local
  hooks:
  - id: poker-quick-test
    name: Poker Assistant Quick Tests
    entry: python poker_smoke.py --quick
    language: system
    always_run: true
    stages: [commit]
  
  - id: poker-unit-tests
    name: Poker Assistant Unit Tests
    entry: python run_all_tests.py --unit
    language: system
    pass_filenames: false
    stages: [push]

- repo: https://github.com/psf/black
  rev: 22.3.0
  hooks:
  - id: black
    language_version: python3

- repo: https://github.com/pycqa/flake8
  rev: 4.0.1
  hooks:
  - id: flake8
    args: [--max-line-length=100, --ignore=E203,W503]
EOF

# Install hooks
pre-commit install
pre-commit install --hook-type pre-push
```

## ğŸ“Š Test Maintenance Schedule

### **Daily (Automated)**
- âœ… **Quick smoke tests** on every commit
- âœ… **Unit tests** on every pull request
- âœ… **Performance monitoring** for regressions

### **Weekly (Manual)**
```bash
# Complete validation with reporting
python final_test_validation.py

# Review test logs for patterns
grep -i "slow\|error\|fail" poker_tests.log

# Update performance baselines if needed
python performance_benchmark_tests.py --update-baselines
```

### **Monthly (Manual)**
```bash
# Deep analysis and cleanup
python run_all_tests.py --coverage --detailed

# Review and update test data
python test_config.py --validate-scenarios

# Clean up old test artifacts
find . -name "test_poker_*.db" -delete
find . -name "validation_report_*.json" -mtime +30 -delete
```

### **Release Preparation**
```bash
# Full comprehensive validation
python final_test_validation.py --comprehensive

# Performance regression analysis
python performance_benchmark_tests.py --compare-with-baseline

# Security audit
python run_all_tests.py --security --strict

# Generate release test report
python final_test_validation.py --release-report
```

## ğŸ”§ Test Configuration Management

### **Performance Thresholds**

Update `test_config.py` when requirements change:

```python
# Adjust thresholds based on target hardware
TEST_CONFIG = TestConfig(
    max_hand_evaluation_time=0.001,    # 1ms for fast machines
    max_equity_calculation_time=3.0,   # 3s for production
    max_analysis_time=0.5,             # 500ms for responsive UI
    max_memory_growth_mb=50.0,         # 50MB for memory efficiency
)
```

### **Test Data Management**

```bash
# Refresh test scenarios
python test_config.py --generate-new-scenarios

# Validate test data integrity
python test_config.py --validate-data

# Export test scenarios for sharing
python test_config.py --export-scenarios test_data.json
```

## ğŸ› Troubleshooting Common Issues

### **Performance Test Failures**

```bash
# Check system load
python -c "import psutil; print(f'CPU: {psutil.cpu_percent()}%, Memory: {psutil.virtual_memory().percent}%')"

# Run isolated performance test
python performance_benchmark_tests.py --isolated

# Adjust thresholds temporarily
python run_all_tests.py --performance --lenient
```

### **Memory Test Failures**

```bash
# Check for memory leaks
python -m pytest enhanced_poker_test.py::TestMemoryAndPerformance -v -s

# Force garbage collection
python -c "import gc; gc.collect(); print('GC completed')"

# Monitor memory during tests
python performance_benchmark_tests.py --monitor-memory
```

### **Concurrency Test Failures**

```bash
# Reduce thread count
export POKER_TEST_MAX_THREADS=2
python run_all_tests.py --enhanced

# Check for deadlocks
timeout 60 python run_all_tests.py --enhanced || echo "Possible deadlock"

# Run with debugging
python -m pytest enhanced_poker_test.py::TestConcurrency -v -s --tb=long
```

### **Database Test Failures**

```bash
# Clean up test databases
find . -name "test_poker_*.db" -delete
rm -f poker_decisions.db

# Check file permissions
ls -la *.db

# Test database recreation
python poker_init.py
```

## ğŸ“ˆ Performance Monitoring

### **Baseline Management**

```python
# Save current performance as baseline
python performance_benchmark_tests.py --save-baseline

# Compare with saved baseline
python performance_benchmark_tests.py --compare-baseline

# Update baseline after optimization
python performance_benchmark_tests.py --update-baseline
```

### **Performance Regression Detection**

Create `performance_monitoring.py`:

```python
#!/usr/bin/env python3
"""
Performance monitoring script for detecting regressions.
Run this periodically to ensure performance doesn't degrade.
"""

import json
import time
from pathlib import Path
from test_config import BenchmarkSuite

def check_performance_regression():
    """Check for performance regressions."""
    baseline_file = Path("performance_baseline.json")
    
    # Run current benchmarks
    benchmark = BenchmarkSuite()
    current_results = {
        'hand_evaluation': benchmark.benchmark_hand_evaluation(),
        'equity_calculation': benchmark.benchmark_equity_calculation(1000),
        'complete_analysis': benchmark.benchmark_complete_analysis()
    }
    
    # Load baseline if exists
    if baseline_file.exists():
        with open(baseline_file) as f:
            baseline = json.load(f)
        
        # Check for regressions
        regressions = []
        for test_name, current in current_results.items():
            if test_name in baseline:
                baseline_mean = baseline[test_name]['mean']
                current_mean = current['mean']
                
                # 20% regression threshold
                if current_mean > baseline_mean * 1.2:
                    regression = (current_mean - baseline_mean) / baseline_mean * 100
                    regressions.append(f"{test_name}: {regression:.1f}% slower")
        
        if regressions:
            print("âš ï¸  Performance regressions detected:")
            for regression in regressions:
                print(f"   {regression}")
            return False
        else:
            print("âœ… No performance regressions detected")
            return True
    else:
        # Save as new baseline
        with open(baseline_file, 'w') as f:
            json.dump(current_results, f, indent=2)
        print("ğŸ“Š Performance baseline saved")
        return True

if __name__ == "__main__":
    check_performance_regression()
```

## ğŸš€ Deployment Integration

### **Staging Environment Validation**

```bash
#!/bin/bash
# staging_validation.sh

echo "ğŸ¯ Staging Environment Validation"

# Run comprehensive tests
python final_test_validation.py --staging

# Check database connectivity
python -c "from poker_init import open_db; db = open_db(); print('âœ… Database OK'); db.close()"

# Verify GUI can start (headless)
python -c "import tkinter; print('âœ… GUI libraries OK')"

# Performance validation
python performance_benchmark_tests.py --staging-thresholds

# Security scan
python run_all_tests.py --security --production-mode

echo "âœ… Staging validation complete"
```

### **Production Readiness Checklist**

Create `production_checklist.py`:

```python
#!/usr/bin/env python3
"""
Production readiness checklist for Poker Assistant.
"""

def production_readiness_check():
    """Run production readiness checks."""
    checks = []
    
    # Test coverage
    try:
        # Run coverage analysis
        import subprocess
        result = subprocess.run(
            ['python', 'run_all_tests.py', '--coverage'],
            capture_output=True, text=True
        )
        if 'failed' not in result.stdout.lower():
            checks.append(("âœ…", "Test Coverage", "All tests pass"))
        else:
            checks.append(("âŒ", "Test Coverage", "Some tests failing"))
    except:
        checks.append(("âš ï¸", "Test Coverage", "Could not run coverage"))
    
    # Performance validation
    try:
        from test_config import BenchmarkSuite
        benchmark = BenchmarkSuite()
        hand_eval = benchmark.benchmark_hand_evaluation()
        
        if hand_eval['mean'] < 0.002:  # 2ms threshold
            checks.append(("âœ…", "Performance", f"Hand eval: {hand_eval['mean']*1000:.1f}ms"))
        else:
            checks.append(("âš ï¸", "Performance", f"Hand eval slow: {hand_eval['mean']*1000:.1f}ms"))
    except:
        checks.append(("âŒ", "Performance", "Could not run benchmarks"))
    
    # Security validation
    try:
        result = subprocess.run(
            ['python', 'run_all_tests.py', '--security'],
            capture_output=True, text=True
        )
        if result.returncode == 0:
            checks.append(("âœ…", "Security", "Security tests pass"))
        else:
            checks.append(("âŒ", "Security", "Security tests fail"))
    except:
        checks.append(("âš ï¸", "Security", "Could not run security tests"))
    
    # Print results
    print("ğŸš€ Production Readiness Checklist")
    print("="*50)
    for status, category, details in checks:
        print(f"{status} {category:<15} {details}")
    
    # Overall assessment
    failed = len([c for c in checks if c[0] == "âŒ"])
    warned = len([c for c in checks if c[0] == "âš ï¸"])
    
    if failed == 0 and warned == 0:
        print("\nğŸ‰ READY FOR PRODUCTION")
        return True
    elif failed == 0:
        print(f"\nâš ï¸  READY WITH WARNINGS ({warned} warnings)")
        return True
    else:
        print(f"\nâŒ NOT READY ({failed} failures, {warned} warnings)")
        return False

if __name__ == "__main__":
    import sys
    ready = production_readiness_check()
    sys.exit(0 if ready else 1)
```

## ğŸ“š Knowledge Base

### **Common Test Patterns**

```python
# Testing new poker logic
def test_new_feature():
    """Template for testing new features."""
    # Arrange - Set up test data
    hole = [Card('A', Suit.SPADE), Card('K', Suit.HEART)]
    board = [Card('Q', Suit.DIAMOND), Card('J', Suit.CLUB), Card('T', Suit.SPADE)]
    
    # Act - Run the function under test
    result = new_poker_function(hole, board)
    
    # Assert - Verify expected behavior
    assert result.decision in ["FOLD", "CALL", "RAISE", "CHECK"]
    assert 0 <= result.equity <= 1
```

### **Performance Test Patterns**

```python
# Testing performance of new code
def test_performance():
    """Template for performance tests."""
    import time
    
    start_time = time.perf_counter()
    
    # Run operation many times
    for _ in range(1000):
        result = expensive_operation()
    
    elapsed_time = time.perf_counter() - start_time
    
    # Performance assertion
    assert elapsed_time < 1.0, f"Too slow: {elapsed_time:.3f}s"
```

### **Integration Test Patterns**

```python
# Testing complete workflows
def test_complete_workflow():
    """Template for integration tests."""
    # Simulate complete poker hand
    hole = generate_random_hole()
    
    # Pre-flop
    preflop_analysis = analyse_hand(hole, [], Position.BTN, 50, 5.0, 2.0, 6)
    
    if preflop_analysis.decision != "FOLD":
        # Continue to flop
        flop = generate_random_flop()
        flop_analysis = analyse_hand(hole, flop, Position.BTN, 50, 15.0, 8.0, 4)
        
        # Validate consistency
        assert flop_analysis.equity >= 0
        assert flop_analysis.decision in VALID_DECISIONS
```

## ğŸ‰ Summary

This comprehensive testing and maintenance framework ensures:

âœ… **Reliable Development Workflow** - Clear processes for daily development  
âœ… **Automated Quality Gates** - CI/CD integration with comprehensive testing  
âœ… **Performance Monitoring** - Regression detection and baseline management  
âœ… **Maintenance Procedures** - Regular health checks and cleanup processes  
âœ… **Production Readiness** - Deployment validation and monitoring  
âœ… **Knowledge Preservation** - Documentation and patterns for future development  

The Poker Assistant now has **enterprise-grade testing infrastructure** that will maintain code quality and system reliability throughout its lifecycle.

---

**Next Steps:**
1. Set up CI/CD pipeline using provided workflows
2. Establish regular maintenance schedule  
3. Configure performance monitoring
4. Train team on testing procedures
5. Begin development with confidence! ğŸš€
